import json
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from bert4torch.models import build_transformer_model
from bert4torch.tokenizers import Tokenizer
from bert4torch.snippets import sequence_padding, ListDataset, seed_everything
from bert4torch.generation import AutoRegressiveDecoder
from tqdm import tqdm
import os
import numpy as np
import types
from rouge import Rouge
from transformers import get_linear_schedule_with_warmup

# é…ç½®å‚æ•°
current_dir = os.path.dirname(os.path.abspath(__file__))

checkpoint_dir = os.path.join(current_dir, 'checkpoint')

config_path = os.path.join(checkpoint_dir, 'config.json')
checkpoint_path = os.path.join(checkpoint_dir, 'pytorch_model.bin')
dict_path = os.path.join(checkpoint_dir, 'vocab.txt')

train_data_path = os.path.join(current_dir, 'data', 'LCSTS_origin', 'train.jsonl')
valid_data_path = os.path.join(current_dir, 'data', 'LCSTS_origin', 'valid.jsonl')

# è¶…å‚æ•°
maxlen = 512 # åŸæ–‡æœ€å¤§é•¿åº¦
max_target_len = 128 # æ‘˜è¦æœ€å¤§é•¿åº¦
batch_size = 32
epochs = 10
learning_rate = 2e-5
device = 'cuda' if torch.cuda.is_available() else 'cpu'

seed_everything(42)
# åˆ†è¯å™¨
tokenizer = Tokenizer(dict_path, do_lower_case=True)

# æ•°æ®å¤„ç†
class SummaryDataset(Dataset):
    def __init__(self, file_path):
        self.data = []
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                self.data.append(json.loads(line))
                # è®­ç»ƒé›†åªå– 50000 æ¡
                if "train.jsonl" in file_path and len(self.data) > 20000: break 
                # éªŒè¯é›†åªå– 2000 æ¡
                if "valid.jsonl" in file_path and len(self.data) > 2000: break

    def __len__(self):
        return len(self.data)

    def __getitem__(self, index):
        item = self.data[index]
        text = item.get('text', '')
        summary = item.get('summary', '')

        # è½¬æ¢ä¸º token ids
        token_ids, _ = tokenizer.encode(text, maxlen=maxlen)
        summary_ids, _ = tokenizer.encode(summary, maxlen=max_target_len)

        return token_ids, summary_ids

def collate_fn(batch):
    """
    å°†ä¸€ä¸ª batch çš„æ•°æ®è¿›è¡Œ padding å¯¹é½
    """
    batch_token_ids, batch_summary_ids = [], []
    for token_ids, summary_ids in batch:
        batch_token_ids.append(token_ids)
        batch_summary_ids.append(summary_ids)

    # padding åˆ°åŒä¸€é•¿åº¦
    batch_token_ids = sequence_padding(batch_token_ids)
    batch_summary_ids = sequence_padding(batch_summary_ids)

    return torch.tensor(batch_token_ids), torch.tensor(batch_summary_ids)

# åŠ è½½æ•°æ®
print("æ­£åœ¨åŠ è½½æ•°æ®...")
train_dataset = SummaryDataset(train_data_path)
valid_dataset = SummaryDataset(valid_data_path)

train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=2, pin_memory=True, shuffle=True, collate_fn=collate_fn)
valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size*2, num_workers=2, pin_memory=True, shuffle=False, collate_fn=collate_fn)

# æ„å»ºæ¨¡å‹
print("æ­£åœ¨åŠ è½½æ¨¡å‹...")

# è¯»å– Hugging Face çš„ config.json
with open(config_path, 'r', encoding='utf-8') as f:
    hf_config = json.load(f)

# æ„å»ºå‚æ•°æ˜ å°„
# å¦åˆ™ä¼šå‡ºç° "missing required positional arguments" é”™è¯¯
bert4torch_args = {
    'model': 'bart', 
    'vocab_size': hf_config['vocab_size'],                  # è¯è¡¨å¤§å°
    'hidden_size': hf_config['d_model'],                    # å¯¹åº” d_model
    'num_hidden_layers': hf_config['encoder_layers'],       # å¯¹åº” encoder_layers
    'num_attention_heads': hf_config['encoder_attention_heads'], # å¯¹åº” encoder_attention_heads
    'intermediate_size': hf_config['encoder_ffn_dim'],      # å¯¹åº” encoder_ffn_dim
    'hidden_act': hf_config['activation_function'],         # å¯¹åº” activation_function
    'dropout_rate': hf_config['dropout'],
    'max_position': hf_config['max_position_embeddings'],
    'segment_vocab_size': 0,
}

print("å·²æ‰‹åŠ¨å®Œæˆé…ç½®å‚æ•°æ˜ å°„ï¼Œå¼€å§‹æ„å»º...")

model = build_transformer_model(
    config_path=None, 
    checkpoint_path=checkpoint_path,
    **bert4torch_args
).to(device)

print("æ¨¡å‹åŠ è½½æˆåŠŸï¼")

# å®šä¹‰ä¼˜åŒ–å™¨
optimizer = optim.Adam(model.parameters(), lr=learning_rate)
# å®šä¹‰æŸå¤±å‡½æ•° (å¿½ç•¥ padding éƒ¨åˆ†çš„ loss)
criterion = nn.CrossEntropyLoss(ignore_index=0)

# ================= ã€æ–°å¢ã€‘å®šä¹‰å­¦ä¹ ç‡è°ƒåº¦å™¨ =================
# 1. è®¡ç®—æ€»è®­ç»ƒæ­¥æ•° = æ¯ä¸ªEpochçš„æ­¥æ•° * æ€»Epochæ•°
total_steps = len(train_dataloader) * epochs

# 2. è®¾å®šé¢„çƒ­æ­¥æ•° (Warmup Steps)
warmup_steps = int(total_steps * 0.01)

# 3. åˆ›å»ºè°ƒåº¦å™¨
scheduler = get_linear_schedule_with_warmup(
    optimizer, 
    num_warmup_steps=warmup_steps, 
    num_training_steps=total_steps
)
# ==========================================================

# å®šä¹‰ç”Ÿæˆå™¨ (ç”¨äºä¸´æ—¶æŸ¥çœ‹æ•ˆæœ)
class ArticleSummaryDecoder(AutoRegressiveDecoder):
    @AutoRegressiveDecoder.wraps(default_rtype='logits')
    def predict(self, inputs, output_ids, states=None):
        """
        è¿™ä¸ªå‡½æ•°ä¼šè¢« beam_search å¾ªç¯è°ƒç”¨ã€‚
        inputs: ä¹Ÿå°±æ˜¯ generate é‡Œä¼ å…¥çš„ [token_ids]
        output_ids: ç›®å‰å·²ç»ç”Ÿæˆçš„ token åºåˆ—
        """
        # è·å–ç¼–ç å™¨è¾“å…¥ (åŸæ–‡)
        token_ids = inputs[0] 
        
        # è°ƒç”¨æ¨¡å‹
        logits = model.predict([token_ids, output_ids])
        
        # è·å–æœ€åä¸€ä¸ª token çš„é¢„æµ‹ç»“æœ
        # BART è¾“å‡ºæ˜¯ [encoder_hidden, decoder_logits]
        # æˆ‘ä»¬è¦å–åˆ—è¡¨æœ€åä¸€ä¸ª(decoder_logits)ï¼Œç„¶åå–åºåˆ—çš„æœ€åä¸€æ­¥(:, -1, :)
        return logits[-1][:, -1, :]

    def generate(self, text, topk=4):
        """
        ç”Ÿæˆå‡½æ•°
        topk: è¿™é‡Œè¡¨ç¤º Beam Search çš„ beam size (é›†æŸæœç´¢å®½åº¦)ï¼Œ4 æ˜¯å¸¸ç”¨å€¼
        """
        # ç¼–ç åŸæ–‡
        token_ids, _ = tokenizer.encode(text, maxlen=maxlen)
        
        # è°ƒç”¨ beam_search (å®ƒä¼šè‡ªåŠ¨å¾ªç¯è°ƒç”¨ä¸Šé¢çš„ predict)
        # è¿™é‡Œçš„ [token_ids] å°±ä¼šä¼ ç»™ predict çš„ inputs
        output_ids = self.beam_search([token_ids], top_k=topk)

        output_ids = output_ids[0]
        # è§£ç ä¸ºæ–‡æœ¬
        return tokenizer.decode(output_ids)

# åˆå§‹åŒ–ç”Ÿæˆå™¨
summary_generator = ArticleSummaryDecoder(
    bos_token_id=tokenizer._token_end_id,
    eos_token_id=tokenizer._token_end_id,
    max_length=max_target_len,
    device=device
)

# è¯„ä¼°
@torch.no_grad()
def evaluate(dataset, limit=100):
    """
    åœ¨éªŒè¯é›†ä¸Šè·‘ä¸€é ROUGE è¯„åˆ†
    limit: ä¸ºäº†é€Ÿåº¦ï¼Œé»˜è®¤åªæµ‹éªŒè¯é›†çš„å‰ 100 æ¡
    """
    print(f"æ­£åœ¨è¿›è¡Œ ROUGE è¯„ä¼° (é‡‡æ · {limit} æ¡)...")
    rouge = Rouge()
    preds, refs = [], []
    
    # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼ (å…³é—­ Dropout)
    model.eval()
    
    for i, item in tqdm(enumerate(dataset.data), total=limit, desc="Evaluating"):
        if i >= limit: break
        
        text = item.get('text', '')
        ref = item.get('summary', '')
        
        if not text or not ref: continue
        
        # ç”Ÿæˆæ‘˜è¦
        pred = summary_generator.generate(text)
        
        # ä¸­æ–‡ ROUGE éœ€è¦æŒ‰å­—åˆ†å¼€
        pred_seg = ' '.join([c for c in pred]) if pred else "ç©º"
        ref_seg = ' '.join([c for c in ref]) if ref else "ç©º"
        
        preds.append(pred_seg)
        refs.append(ref_seg)
    
    # è®¡ç®—åˆ†æ•°
    if preds:
        scores = rouge.get_scores(preds, refs, avg=True)
        return scores
    return None


# è®­ç»ƒ
def train():
    print(f"å¼€å§‹è®­ç»ƒï¼Œè®¾å¤‡: {device}")
    
    for epoch in range(epochs):
        model.train()
        total_loss = 0
        progress_bar = tqdm(train_dataloader, desc=f'Epoch {epoch+1}/{epochs}')
        
        for step, (batch_x, batch_y) in enumerate(progress_bar):
            batch_x, batch_y = batch_x.to(device).long(), batch_y.to(device).long()
            
            # BART è®­ç»ƒè¾“å…¥:
            # Encoder Input: batch_x (åŸæ–‡)
            # Decoder Input: batch_y[:, :-1] (æ‘˜è¦å»æ‰æœ€åä¸€ä¸ªå­—)
            # Label: batch_y[:, 1:] (æ‘˜è¦å»æ‰ç¬¬ä¸€ä¸ªå­—ï¼Œå‘åç§»ä¸€ä½)
            
            optimizer.zero_grad()
            
            # bert4torch çš„ BART è°ƒç”¨æ–¹å¼ï¼šä¼ å…¥ list [src_ids, tgt_ids]
            # è¿™é‡Œæˆ‘ä»¬éœ€è¦æ‰‹åŠ¨æ„é€  decoder input
            decoder_input = batch_y[:, :-1]
            labels = batch_y[:, 1:]
            
            # å‰å‘ä¼ æ’­
            model_outputs = model([batch_x, decoder_input])
            logits = model_outputs[-1]
            
            # è®¡ç®— Loss (éœ€è¦æŠŠ logits å±•å¹³)
            loss = criterion(logits.reshape(-1, logits.shape[-1]), labels.reshape(-1))
            
            loss.backward()
            optimizer.step()
            scheduler.step()  # --- æ¯ä¸ª step æ›´æ–°å­¦ä¹ ç‡ ---
            
            current_lr = optimizer.param_groups[0]['lr']
            total_loss += loss.item()
            progress_bar.set_postfix(loss=total_loss/(step+1), lr=current_lr)
        
        # --- æ¯ä¸ª Epoch ç»“æŸåçš„å·¥ä½œ ---
        avg_loss = total_loss / len(train_dataloader)
        print(f"\nEpoch {epoch+1} è®­ç»ƒå®Œæˆ å¹³å‡ Loss: {avg_loss:.4f}")
        
        # ä¿å­˜æƒé‡
        save_path = f'model/bart_epoch_{epoch+1}.pt'
        torch.save(model.state_dict(), save_path)
        print(f"âœ… æ¨¡å‹å·²ä¿å­˜: {save_path}")

        # è¿è¡Œè¯„ä¼°
        scores = evaluate(valid_dataset)
        
        if scores:
            print(f"ğŸ“Š Epoch {epoch+1} éªŒè¯é›†å¾—åˆ†:")
            print(f"   ROUGE-1: {scores['rouge-1']['f'] * 100:.2f}")
            print(f"   ROUGE-2: {scores['rouge-2']['f'] * 100:.2f}")
            print(f"   ROUGE-L: {scores['rouge-l']['f'] * 100:.2f}")


if __name__ == '__main__':
    # ç¡®ä¿ model ç›®å½•å­˜åœ¨
    if not os.path.exists('model'):
        os.makedirs('model')
    train()