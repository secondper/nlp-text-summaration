import os
import json
import torch
import numpy as np
import gradio as gr
from bert4torch.models import build_transformer_model
from bert4torch.tokenizers import Tokenizer
from bert4torch.generation import AutoRegressiveDecoder

# é…ç½®
current_dir = os.path.dirname(os.path.abspath(__file__))
checkpoint_dir = os.path.join(current_dir, 'checkpoint')
model_dir = os.path.join(current_dir, 'model')

config_path = os.path.join(checkpoint_dir, 'config.json')
dict_path = os.path.join(checkpoint_dir, 'vocab.txt')

# åŠ è½½æƒé‡çš„è·¯å¾„
weight_path = os.path.join(model_dir, 'bart_epoch_4.pt') 

device = 'cuda' if torch.cuda.is_available() else 'cpu'
maxlen = 512
max_target_len = 128

print(f"æ­£åœ¨å¯åŠ¨ç½‘é¡µç«¯... è®¾å¤‡: {device}")

# åŠ è½½æ¨¡å‹
tokenizer = Tokenizer(dict_path, do_lower_case=True)

with open(config_path, 'r', encoding='utf-8') as f:
    hf_config = json.load(f)

bert4torch_args = {
    'model': 'bart', 
    'vocab_size': hf_config['vocab_size'],
    'hidden_size': hf_config['d_model'],
    'num_hidden_layers': hf_config['encoder_layers'],
    'num_attention_heads': hf_config['encoder_attention_heads'],
    'intermediate_size': hf_config['encoder_ffn_dim'],
    'hidden_act': hf_config['activation_function'],
    'dropout_rate': hf_config['dropout'],
    'max_position': hf_config['max_position_embeddings'],
    'segment_vocab_size': 0,
}

model = build_transformer_model(config_path=None, checkpoint_path=None, **bert4torch_args).to(device)

if os.path.exists(weight_path):
    model.load_state_dict(torch.load(weight_path, map_location=device))
    model.eval()
    print("æ¨¡å‹æƒé‡åŠ è½½æˆåŠŸï¼")
else:
    print(f"æ‰¾ä¸åˆ°æƒé‡: {weight_path}")

# å®šä¹‰æ¨ç†é€»è¾‘
class ArticleSummaryDecoder(AutoRegressiveDecoder):
    @AutoRegressiveDecoder.wraps(default_rtype='logits')
    def predict(self, inputs, output_ids, states=None):
        token_ids = inputs[0] 
        logits = model.predict([token_ids, output_ids])
        return logits[-1][:, -1, :]

    def generate(self, text, topk=4):
        token_ids, _ = tokenizer.encode(text, maxlen=maxlen)
        output_ids = self.beam_search([token_ids], top_k=topk)
        
        # æ ¼å¼æ¸…æ´—
        output_ids = output_ids[0]
            
        return tokenizer.decode(output_ids)

summary_generator = ArticleSummaryDecoder(
    bos_token_id=tokenizer._token_end_id,
    eos_token_id=tokenizer._token_end_id,
    max_length=max_target_len,
    device=device
)

def predict_fn(text):
    if not text: return "è¯·è¾“å…¥å†…å®¹..."
    return summary_generator.generate(text)

# æ­å»º Gradio ç•Œé¢
# è¿™é‡Œå®šä¹‰é¡µé¢çš„æ ·å¼å’Œäº¤äº’
with gr.Blocks(title="æ™ºèƒ½æ‘˜è¦ç”Ÿæˆç³»ç»Ÿ", theme=gr.themes.Soft()) as demo:
    
    # æ ‡é¢˜éƒ¨åˆ†
    gr.Markdown("# æ™ºèƒ½æ‘˜è¦ç”Ÿæˆç³»ç»Ÿ")
    gr.Markdown("æœ¬ç³»ç»ŸåŸºäº **OpenMOSS-Team/bart-base-chinese** æ¨¡å‹å¾®è°ƒï¼Œèƒ½å¤Ÿè‡ªåŠ¨æå–å•†å“æ ¸å¿ƒå–ç‚¹æˆ–ç”Ÿæˆæ–°é—»æ ‡é¢˜ã€‚")

    # å·¦å³å¸ƒå±€
    with gr.Row():
        with gr.Column():
            # å·¦è¾¹ï¼šè¾“å…¥
            input_text = gr.Textbox(
                label="è¾“å…¥æ–‡æœ¬",
                placeholder="è¯·ç²˜è´´å•†å“è¯¦æƒ…æè¿°æˆ–æ–°é—»é•¿æ–‡æœ¬...",
                lines=10
            )
            # æŒ‰é’®
            submit_btn = gr.Button("âœ¨ ç”Ÿæˆæ‘˜è¦", variant="primary")
            
            # æ¸…é™¤æŒ‰é’®
            clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©º")

        with gr.Column():
            # å³è¾¹ï¼šè¾“å‡º
            output_text = gr.Textbox(
                label="ç”Ÿæˆç»“æœ",
                lines=5
            )

    # åº•éƒ¨ï¼šç¤ºä¾‹ (è¿™ç‚¹éå¸¸åŠ åˆ†ï¼åŠ©æ•™ç‚¹ä¸€ä¸‹å°±èƒ½çœ‹æ•ˆæœ)
    gr.Markdown("### ğŸ“ ç‚¹å‡»ä¸‹æ–¹ç¤ºä¾‹å¿«é€Ÿæµ‹è¯•")
    gr.Examples(
        examples=[
            ["2007å¹´ä¹”å¸ƒæ–¯å‘äººä»¬å±•ç¤ºiPhoneå¹¶å®£ç§°â€œå®ƒå°†ä¼šæ”¹å˜ä¸–ç•Œâ€ï¼Œè¿˜æœ‰äººè®¤ä¸ºä»–åœ¨å¤¸å¤§å…¶è¯ï¼Œç„¶è€Œåœ¨8å¹´åï¼Œä»¥iPhoneä¸ºä»£è¡¨çš„è§¦å±æ™ºèƒ½æ‰‹æœºå·²ç»å¸­å·å…¨çƒå„ä¸ªè§’è½ã€‚æœªæ¥ï¼Œæ™ºèƒ½æ‰‹æœºå°†ä¼šæˆä¸ºâ€œçœŸæ­£çš„ä¸ªäººç”µè„‘â€ï¼Œä¸ºäººç±»å‘å±•åšå‡ºæ›´å¤§çš„è´¡çŒ®ã€‚"],
            ["é•¿æœŸé¥®ç”¨è¿‡çƒ«çš„é¥®å“ï¼ˆè¶…è¿‡65Â°Cï¼‰å·²è¢«ä¸–ç•Œå«ç”Ÿç»„ç»‡åˆ—ä¸ºæ˜ç¡®çš„è‡´ç™Œé£é™©å› ç´ ã€‚é«˜æ¸©ä¼šåå¤ç¼ä¼¤é£Ÿé“é»è†œï¼Œå¼•å‘æ…¢æ€§ç‚ç—‡ï¼Œä»è€Œå¯èƒ½å¢åŠ é£Ÿç®¡ç™Œå˜å‡ ç‡ã€‚ä¸“å®¶å»ºè®®ï¼Œå°†çƒ­é¥®æ™¾ç½®ç‰‡åˆ»ï¼Œå¾…æ¸©çƒ­é€‚å£æ—¶å†é¥®ç”¨ï¼Œæ˜¯ç®€å•æœ‰æ•ˆçš„ä¿æŠ¤ä¹ æƒ¯ã€‚"],
            ["å¤©æ–‡å­¦å®¶é€šè¿‡è©¹å§†æ–¯Â·éŸ¦ä¼¯å¤ªç©ºæœ›è¿œé•œï¼Œåœ¨ä¸€é¢—ç³»å¤–è¡Œæ˜Ÿçš„å¤§æ°”ä¸­é¦–æ¬¡æ¸…æ™°æ¢æµ‹åˆ°ç”²çƒ·ä¸äºŒæ°§åŒ–ç¢³å­˜åœ¨çš„è¿¹è±¡ã€‚è¯¥è¡Œæ˜Ÿä½äºå®œå±…å¸¦ï¼Œè¿™ä¸€å‘ç°ä¸ºå¯»æ‰¾åœ°å¤–ç”Ÿå‘½æä¾›äº†å…³é”®æ•°æ®ï¼Œæ˜¯ç³»å¤–è¡Œæ˜Ÿç ”ç©¶é¢†åŸŸçš„ä¸€é¡¹é‡å¤§çªç ´ã€‚"]
        ],
        inputs=input_text,
        outputs=output_text,
        fn=predict_fn,
        cache_examples=False,
    )

    # ç»‘å®šäº‹ä»¶
    submit_btn.click(fn=predict_fn, inputs=input_text, outputs=output_text)
    clear_btn.click(lambda: ("", ""), outputs=[input_text, output_text])

# å¯åŠ¨æœåŠ¡
if __name__ == "__main__":
    # launch ä¼šè‡ªåŠ¨åœ¨æœ¬åœ°å¯åŠ¨ä¸€ä¸ªç½‘é¡µæœåŠ¡å™¨
    demo.launch(server_name="127.0.0.1", server_port=7860, inbrowser=True, share=True)