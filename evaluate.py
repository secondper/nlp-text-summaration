import json
import os
from rouge import Rouge
from tqdm import tqdm

# ================= é…ç½® =================
current_dir = os.path.dirname(os.path.abspath(__file__))
# è¿™æ˜¯ä½ ä¸Šä¸€æ­¥ç”Ÿæˆçš„é¢„æµ‹æ–‡ä»¶
prediction_file = os.path.join(current_dir, 'result', 'test_predictions.jsonl')

def load_data(filename):
    """è¯»å–é¢„æµ‹æ–‡ä»¶ï¼Œæå–é¢„æµ‹ç»“æœå’Œæ ‡å‡†æ ‡ç­¾"""
    preds = []
    refs = []
    
    if not os.path.exists(filename):
        print(f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶: {filename}")
        return [], []

    print(f"æ­£åœ¨è¯»å–æ–‡ä»¶: {filename} ...")
    with open(filename, 'r', encoding='utf-8') as f:
        for line in tqdm(f):
            try:
                item = json.loads(line)
                p = item.get('predict', '')
                r = item.get('label', '') # ä¹‹å‰çš„è„šæœ¬é‡Œæˆ‘ä»¬æŠŠå‚è€ƒæ‘˜è¦å­˜ä¸ºäº† label

                # è¿‡æ»¤æ‰ç©ºçš„æ•°æ®ï¼Œé˜²æ­¢æŠ¥é”™
                if p and r:
                    # ã€å…³é”®æ­¥éª¤ã€‘ä¸­æ–‡è¯„æµ‹éœ€è¦æŠŠæ¯ä¸ªå­—ç”¨ç©ºæ ¼éš”å¼€
                    # å¦åˆ™ rouge ä¼šæŠŠæ•´å¥è¯å½“æˆä¸€ä¸ªå•è¯
                    p_seg = ' '.join([char for char in p])
                    r_seg = ' '.join([char for char in r])
                    
                    preds.append(p_seg)
                    refs.append(r_seg)
            except:
                continue
    
    return preds, refs

def evaluate():
    # 1. åŠ è½½æ•°æ®
    preds, refs = load_data(prediction_file)
    
    if not preds:
        print("æ²¡æœ‰è¯»åˆ°æœ‰æ•ˆæ•°æ®ï¼Œè¯·æ£€æŸ¥ test_predictions.jsonl æ˜¯å¦ç”ŸæˆæˆåŠŸï¼Œä¸”åŒ…å« 'label' å­—æ®µã€‚")
        return

    print(f"æœ‰æ•ˆæ ·æœ¬æ•°: {len(preds)}")
    print("æ­£åœ¨è®¡ç®— ROUGE åˆ†æ•°...")

    # 2. åˆå§‹åŒ– ROUGE è®¡ç®—å™¨
    rouge = Rouge()
    
    # 3. è®¡ç®—åˆ†æ•° (avg=True è¡¨ç¤ºå–å¹³å‡å€¼)
    scores = rouge.get_scores(preds, refs, avg=True)

    # 4. æ‰“å°ç»“æœ
    print("\n" + "="*40)
    print("è¯„ä¼°ç»“æœ (ROUGE Score)")
    print("="*40)
    
    # æ ¼å¼åŒ–æ‰“å°
    def print_metric(name, metrics):
        print(f"ã€{name}ã€‘:")
        print(f"  - Recall (å¬å›ç‡):    {metrics['r']*100:.2f}%")
        print(f"  - Precision (å‡†ç¡®ç‡): {metrics['p']*100:.2f}%")
        print(f"  - F1-Score (ç»¼åˆåˆ†):  {metrics['f']*100:.2f}%")
        print("-" * 20)

    print_metric("ROUGE-1 (å•å­—é‡åˆåº¦)", scores['rouge-1'])
    print_metric("ROUGE-2 (è¯ç»„/äºŒå…ƒé‡åˆåº¦)", scores['rouge-2'])
    print_metric("ROUGE-L (æœ€é•¿å…¬å…±å­åºåˆ—/å¥å­ç»“æ„)", scores['rouge-l'])
    print("="*40)
    
    print("\nğŸ’¡ ç»“æœåˆ†ææç¤ºï¼š")
    print("ROUGE-1: è¡¡é‡ä¿¡æ¯è¦†ç›–åº¦ï¼Œè¶Šé«˜è¯´æ˜å…³é”®è¯æŠ“å¾—è¶Šå‡†ã€‚")
    print("ROUGE-2: è¡¡é‡æµç•…åº¦ï¼Œè¶Šé«˜è¯´æ˜ç”Ÿæˆçš„çŸ­è¯­è¶Šè¿è´¯ã€‚")
    print("ROUGE-L: è¡¡é‡å¥å­ç»“æ„ç›¸ä¼¼åº¦ã€‚")

if __name__ == "__main__":
    evaluate()